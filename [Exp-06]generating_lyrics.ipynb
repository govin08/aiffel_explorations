{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1ade94",
   "metadata": {},
   "source": [
    "**루브릭 1. 데이터의 전처리 및 구성과정이 체계적으로 진행되었는가?**  \n",
    "특수문자 제거, 토크나이저 생성, 패딩 처리의 작업들이 빠짐없이 진행되었는가?\n",
    "\n",
    "-> 잘 진행되었습니다.\n",
    "\n",
    "**루브릭 2. 가사 텍스트 생성 모델이 정상적으로 동작하는가?**  \n",
    "텍스트 제너레이션 결과로 생성된 문장이 해석 가능한 문장인가?\n",
    "\n",
    "-> \n",
    "\n",
    "**루브릭 3. 텍스트 생성모델이 안정적으로 학습되었는가?**  \n",
    "텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n",
    "\n",
    "-> \n",
    "\n",
    "**참고사항**\n",
    "\n",
    "1. 목차(table of contents) 관련  \n",
    "\n",
    "이전 exploration(05)에서는 jupyter notebook extension의 table of contents(2) 기능을 사용하여 전체 문서를 구조화했습니다.\n",
    "이렇게 하면, 굳이 장번호와 절번호를 매번 명시하지 않아도, 장과 절의 순서가 잘 배열되는 동시에, 목차 또한 생성되어 좋습니다.\n",
    "하지만, 이 기능은 로컬 환경의 jupyter notebook에서만 작용하고, (1) 깃허브에 올라갈 때에는 반영되지 않고, (2) AIFFEL jupyter 환경에서도 작동하지 않으며, (3) colab에서도, 단지 장과 절의 hierarchy만 명시될 뿐 번호가 자동적으로 생성되지 않습니다.\n",
    "\n",
    "이에, 이번에는 문서를 다 작성한 후 목차(table of contents)를 직접(manual 하게) 상단에 적어보았습니다.\n",
    "이렇게 하면, 로컬 환경의 jupyter notebook을 사용해 table of contents(2) 기능을 사용할 경우, 문서가 더 이상하게 보이리라는 단점이 있습니다.\n",
    "\n",
    "향후에, AIFFEL jupyter 환경에서도 (table of contents(2) 기능을 포함한) nbextension을 사용할 수 있다면 참 좋을 것 같습니다.\n",
    "\n",
    "2. 코드 및 주석 관련\n",
    "\n",
    "기존 코드에서 이 문서로 코드를 옮길 때에 최대한 간결하게 쓰려고 했습니다.\n",
    "따라서 기존 코드에 붙어있던 주석들은 최대한 제거했습니다.\n",
    "또한, 이 문서에 직접적으로 필요하지 않은 코드들은 가급적 배제하였습니다.\n",
    "다만, `dataset`이라고 이름 붙여졌던 객체에 대해서는, 이 문서의 전체적인 흐름과는 직접적으로 연관이 있지 않음에도 불구하고, 조금 분석을 해보았습니다.\n",
    "\n",
    "또한, 각 장(1. 사전작업, 2. 모델 생성, 훈련 및 평가)의 맨 앞쪽에 주석들을 추가해, 코드에 대한 설명을 적어보았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd808103",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc10844",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "### 1. 사전작업\n",
    "#### 1.1. 모듈 불러오기\n",
    "#### 1.2. 데이터 불러오기\n",
    "#### 1.3. 데이터 정제하기\n",
    "#### 1.4. 데이터 분리하기\n",
    "#### 1.5. `dataset`에 대한 이해\n",
    "### 2. 모델 구성, 훈련 및 평가\n",
    "#### 2.1. 모델 구성\n",
    "#### 2.2. 모델 훈련 및 평가\n",
    "#### 2.3. 문장 생성\n",
    "### 3. 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc287c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e718992c",
   "metadata": {},
   "source": [
    "# 1. 사전작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b71687",
   "metadata": {},
   "source": [
    "파일트리\n",
    "\n",
    "Github\n",
    "\n",
    "├── aiffel_explorations\n",
    "\n",
    "    ├── .git\n",
    "    \n",
    "    └── 06.ipynb\n",
    "\n",
    "└── data\n",
    "\n",
    "    └── exploration_06\n",
    "\n",
    "        ├── .git\n",
    "\n",
    "            ├── adele.txt\n",
    "\n",
    "            ├── al-green.txt\n",
    "\n",
    "            ː\n",
    "\n",
    "            └── r-kelly.txt\n",
    "        └── lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe9ab9",
   "metadata": {},
   "source": [
    " - `raw_corpus`\n",
    "  - 원래 그대로 형태의 문장들을 불러왔습니다(`list`).\n",
    "  - `txt_file_path`로 파일 경로를 불러왔습니다.\n",
    "  - `raw_corpus`에 담긴 문장들은 총 187088개 입니다.\n",
    " - `corpus`\n",
    "  - `raw_corpus`에 있는 문장들 중 일부 문장들을 제외하고 적당히 정제하여 담았습니다(`list`).\n",
    "  - LMS에 제시되어있는 함수 `preprocess_sentence`를 그대로 가져와서 정제하는 데에 사용했습니다.\n",
    "  - 제외한 문장들은 다음과 같습니다.\n",
    "    - 아무 것도 없는 문장\n",
    "    - 정제한 후, 문장에 포함된 단어의 개수가 15개 이상인 문장\n",
    "  - `corpus`에 담긴 문장들은 총 156227개 입니다.\n",
    " - `tensor`\n",
    "  - `corpus`의 각 문장들을 토큰화했습니다. (`np.array`)\n",
    "  - LMS에 제시되어있는 함수 `tokenizer`를 그대로 가져오되,`num_words`를 7000에서 1200으로 변경했습니다.\n",
    "  - `padding`은 원래 주어졌던 대로 'post'로 설정했습니다.\n",
    "  'pre'로 바꿔보았는데, 성능이 떨어졌습니다.\n",
    " - `src_input`, `tgt_input`\n",
    "  - 각 문장의 맨 뒷단어를 잘라내어 `src_input`을 만들고, 각 문장의 맨 앞단어를 잘라내어 `tgt_input`을 만들었습니다.\n",
    " - `enc_train`, `enc_val`, `dec_train`, `dec_val`\n",
    "  - 데이터를 trainset과 testset, input과 label로 나누었습니다.\n",
    " - `dataset`\n",
    "  - 이 부분은 이번 exploration의 흐름과는 관련이 없는 부분이지만, `dataset`이라는 객체에 대한 궁금증에 여러 실험을 해본 것입니다.\n",
    "  - LMS 노드에 적힌 대로 `dataset`을 정의한 후, 이것의 구조를 들여다보았습니다.\n",
    "  `dataset`은 `tf.data.Dataset`와 같은 데이터타입을 가지고 있습니다.\n",
    "  따라서 구조를 들여다보기 어려우나, iterator의 역할을 하는 것 같아 list comprehension을 통해 `list`형식으로 만들고, 형태를 보기 위해 `np.array`로 바꾸었습니다.\n",
    "  - 그 후, shape을 출력해보니, (배치작업을 하기 전에는) (156227, 2, 14)로 나옵니다.\n",
    "  156227은 문장의 개수를, 2는 `src`/`tgt`을 의미하며, 14는 각 문장의 단어의 개수를 의미합니다.\n",
    "  가공한 후의 shape은 (배치작업 후에는) (610, 2, 256, 14)로 바뀝니다.\n",
    "  여기서 610은 한 epoch당 optimizer가 동작하는 횟수을 의미하고, 256은 배치 사이즈를 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98324c7",
   "metadata": {},
   "source": [
    "## 1.1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01089ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, re, glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc94b4",
   "metadata": {},
   "source": [
    "## 1.2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08505cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_path = '../data/exploration_06/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5912e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(len(txt_list))\n",
    "print(np.array(txt_list[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690cf8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "print(type(raw_corpus))\n",
    "print(len(raw_corpus))\n",
    "print(np.array(raw_corpus[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dca7f9",
   "metadata": {},
   "source": [
    "## 1.3. 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fe33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7747c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "#    if sentence[-1] == \":\": continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split(\" \")) > 15: continue\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6a64f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc6f38e",
   "metadata": {},
   "source": [
    "## 1.4. 데이터 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6285ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tensor,tokenizer)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor, tokenizer\n\u001b[1;32m---> 10\u001b[0m tensor, tokenizer \u001b[38;5;241m=\u001b[39m tokenize(corpus)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(tensor))\n",
      "Cell \u001b[1;32mIn [9], line 6\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(corpus)\n\u001b[0;32m      5\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(corpus)\n\u001b[1;32m----> 6\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor,tokenizer)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor, tokenizer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ex\\lib\\site-packages\\keras\\utils\\data_utils.py:1054\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1049\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sequences` must be a list of iterables. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1050\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound non-iterable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1051\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1054\u001b[0m     maxlen \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m is_dtype_str \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39missubdtype(dtype, np\u001b[38;5;241m.\u001b[39mstr_) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\n\u001b[0;32m   1057\u001b[0m     dtype, np\u001b[38;5;241m.\u001b[39municode_\n\u001b[0;32m   1058\u001b[0m )\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_str:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ex\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2679\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2680\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2794\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ex\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, filters=' ',oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tensor, padding='post')\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "print(type(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d052a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input, test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709c76d",
   "metadata": {},
   "source": [
    "## 1.5. `dataset`에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b764b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "print('BUFFER_SIZE :',BUFFER_SIZE)\n",
    "BATCH_SIZE = 256\n",
    "print('BATCH_SIZE :',BATCH_SIZE)\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "print('steps_per_epoch :',steps_per_epoch)\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "print('VOCAB_SIZE :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a14f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset_as_np_array = np.array([data for data in dataset])\n",
    "print(dataset_as_np_array.shape)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset_as_np_array = np.array([data for data in dataset])\n",
    "print(dataset_as_np_array.shape)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_as_np_array = np.array([data for data in dataset])\n",
    "print(dataset_as_np_array.shape)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821cd1b",
   "metadata": {},
   "source": [
    "# 2. 모델 구성, 훈련 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988835a0",
   "metadata": {},
   "source": [
    " - `TextGenerator`\n",
    "  - 모델을 구성하기 전, `TextGenerator` 클래스를 정의했습니다.\n",
    "  이 정의는 LMS에 정의된 코드를 그대로 따랐습니다.\n",
    "  - 이 클래스는, embedding layer 한 개와 LSTM 레이어 두 개, 그리고 fully connected layer 한 개로 구성되어 있습니다.\n",
    "  - LSTM을 두 개 쌓고 있으며, 그 이후에는 FC layer가 나오므로, `return_sequences`는 둘 다 `True`로 놓습니다.\n",
    " - `model`\n",
    "  - `TextGenerator`를 사용하여 모델을 만듭니다.\n",
    "  - `embedding_size`는 512로 둡니다.\n",
    "  이것은 word vector의 차원을 말합니다.\n",
    "  - `hidden_size`는 2048로 둡니다.\n",
    "  이것은 LSTM의 hidden state vector의 차원을 말합니다.\n",
    "  - 이 두 값은 원래, 각각 256, 1024로 되어 있었지만, 원하는 만큼의 성능이 나오지 않아 둘 다 두 배로 늘린 것입니다.\n",
    " - `prd_sample`\n",
    "  - LMS에서 한 대로, `dataset`의 `take` 메서드를 이용하여 하나의 sample을 가지고 `model`에 입력함으로써 `model.build()`를 호출합니다.\n",
    "  - 그 입력에 대한 결과는 `prd_sample`로 임의로 저장해보았습니다.\n",
    "  - `prd_sample`을 `np.array`로 바꾸어 shape을 계산해보면, (256, 14, 12001)이 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bdc123",
   "metadata": {},
   "source": [
    "## 2.1. 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "prd_sample = model(src_sample)\n",
    "print(type(prd_sample))\n",
    "print(np.array(prd_sample).shape)\n",
    "#print(prd_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c638d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932be2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc365c",
   "metadata": {},
   "source": [
    "## 2.2. 모델 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc3ab0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist = model.fit(enc_train, dec_train,batch_size=256,\n",
    "                 validation_data = (enc_val,dec_val), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693671e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559d6be",
   "metadata": {},
   "source": [
    "## 2.3. 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505cba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71e487",
   "metadata": {},
   "source": [
    "# 3. 회고"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
