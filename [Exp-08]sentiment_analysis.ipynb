{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885c1a60",
   "metadata": {},
   "source": [
    "**루브릭 1. 다양한 방법으로 Text Classification 태스크를 성공적으로 구현하였다.**  \n",
    "3가지 이상의 모델이 성공적으로 시도됨\n",
    "-> \n",
    "\n",
    "**루브릭 2. `gensim`을 활용하여 자체학습된 혹은 사전학습된 임베딩 레이어를 분석하였다.**  \n",
    "gensim의 유사단어 찾기를 활용하여 자체학습한 임베딩과 사전학습 임베딩을 비교 분석함\n",
    "\n",
    "-> \n",
    "\n",
    "**루브릭 3. 한국어 Word2Vec을 활용하여 가시적인 성능향상을 달성했다.**  \n",
    "네이버 영화리뷰 데이터 감성분석 정확도를 85% 이상 달성함\n",
    "\n",
    "-> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a095ef",
   "metadata": {},
   "source": [
    "2022/10/19 (수)  \n",
    " - 마감기한은 어제였습니다만, 다 마무리하지 못했습니다.\n",
    " - github 주소를 일단 올려놓은 상태이고, 추가적으로 작업하고 있습니다.\n",
    " - 오늘 안으로는 어떻게든 완성본으로 바꾸어놓겠습니다.\n",
    " - 마감기한까지 내지 못하였으니 바로 채점을 하셔도 될 것이고 (아마 루브릭을 하나도 받지 못할 것 같습니다.) 혹시라도 감점을 감수하고 조금 더 내용을 채울 수 있다고 하면, 오늘까지는 완성본으로 만들어놓겠습니다.\n",
    " - 어떻게 채점하시건 상관없이, 제 공부를 위해, 이 깃허브 주소의 jupyter notebook은 계속 업데이트해나갈 예정입니다.\n",
    " - 번거롭게 해드려 죄송합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "176df2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e4ff1",
   "metadata": {},
   "source": [
    "## 8-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ac1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4efd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635eec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03251557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b4b9e",
   "metadata": {},
   "source": [
    "## 8-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0a564",
   "metadata": {},
   "source": [
    "## 8-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model1.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model1.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model2.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model2.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model2.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model2.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model3.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model3.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model3.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5a725",
   "metadata": {},
   "source": [
    "## 8-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaebe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47879cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e52264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2506fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37604212",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a28213",
   "metadata": {},
   "source": [
    "## 8-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5036ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=9  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "hist = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57acf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3efa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abdfe2",
   "metadata": {},
   "source": [
    "## 8-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448234ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee769b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99702d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ba601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원 수 \n",
    "\n",
    "# 모델 구성\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f538a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "hist = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d453d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90acd2d",
   "metadata": {},
   "source": [
    "# 1. 사전작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997d6f7",
   "metadata": {},
   "source": [
    "## 1.1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c4ff2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591fa73e",
   "metadata": {},
   "source": [
    "## 1.2. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38314d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176565d",
   "metadata": {},
   "source": [
    "## 1.3. 중복된 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c419e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(150000, 3)\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(test_data))\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd5680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19675b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146183, 3)\n",
      "(49158, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a571e",
   "metadata": {},
   "source": [
    "## 1.4. 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c911ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    1\n",
      "label       0\n",
      "dtype: int64\n",
      "id          0\n",
      "document    1\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19b9608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>2172111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id document  label\n",
       "25857  2172111      NaN      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data['document'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d2f651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5746</th>\n",
       "      <td>402110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id document  label\n",
       "5746  402110      NaN      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.loc[test_data['document'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9ec61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any')\n",
    "test_data = test_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413ee5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n",
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a0f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 3)\n",
      "(49157, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f682e4",
   "metadata": {},
   "source": [
    "## 1.5. 온점 및 구두점 등 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb40d5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d478fb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1321/2798799149.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "/tmp/ipykernel_1321/2798799149.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc2b796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1321/4202506870.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
      "/tmp/ipykernel_1321/4202506870.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id            0\n",
      "document    789\n",
      "label         0\n",
      "dtype: int64\n",
      "id            0\n",
      "document    305\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7153c27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145393, 3)\n",
      "(48852, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any')\n",
    "test_data = test_data.dropna(how = 'any')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc369c68",
   "metadata": {},
   "source": [
    "## 1.6. 불용어(stopwords) 제거 및 형태소 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de157c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be31763c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6474b6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145393/145393 [07:05<00:00, 341.87it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9edbac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['아', '더빙', '진짜', '짜증나다', '목소리'], ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다'], ['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4828a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48852/48852 [02:37<00:00, 310.07it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff980814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_train_copied = copy.deepcopy(X_train)\n",
    "X_test_copied = copy.deepcopy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2fc29f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "145393\n",
      "48852\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_test))\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c029224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test의 실행시간이 너무 많이 걸리므로, X_train과 X_test를\n",
    "# 다시 초기화하고 싶으면 이 셀을 실행하면 된다.\n",
    "X_train = X_train_copied\n",
    "X_test = X_test_copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db6eb0",
   "metadata": {},
   "source": [
    "## 1.7. `word_to_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2b651e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "054d27a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "영화\n"
     ]
    }
   ],
   "source": [
    "word_to_index = tokenizer.word_index\n",
    "word_to_index_copied = copy.deepcopy(word_to_index)\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(word_to_index['영화'])\n",
    "print(index_to_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "650150d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "43752\n"
     ]
    }
   ],
   "source": [
    "print(type(word_to_index))\n",
    "print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4db21663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 43752\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 24337\n",
      "단어 집합에서 희귀 단어의 비율: 0.5562488571950996\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.018715872104872903\n",
      "19415\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "frequent_words=[]\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "    else:\n",
    "        frequent_words.append(key)\n",
    "\n",
    "word_to_index = {key:word_to_index[key] for key in frequent_words}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt))\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq))\n",
    "print(len(frequent_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cd20f3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19415\n",
      "19415\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_index))\n",
    "print(len(index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "789bf11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 19416\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138ac31",
   "metadata": {},
   "source": [
    "## 1.8. 토큰화 (words → vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "909139d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['아', '더빙', '진짜', '짜증나다', '목소리']\n",
      "['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다']\n",
      "['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다']\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train[0])\n",
    "print(X_train[1])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "61296bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2a0d43f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[50, 454, 16, 260, 659]\n",
      "[933, 457, 41, 602, 1, 214, 1449, 24, 961, 675, 19]\n",
      "[386, 2444, 2315, 5671, 2, 222, 9]\n",
      "145393\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train[0])\n",
    "print(X_train[1])\n",
    "print(X_train[2])\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ba98392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "020cb4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145393\n",
      "48852\n",
      "145393\n",
      "48852\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "806cf02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) < 1]\n",
    "print(len(drop_train))\n",
    "print(len(drop_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ad9646f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145162\n",
      "145162\n",
      "48745\n",
      "48745\n"
     ]
    }
   ],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "X_test = np.delete(X_test, drop_test, axis=0)\n",
    "y_test = np.delete(y_test, drop_test, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af5536ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 69\n",
      "리뷰의 평균 길이 : 10.812485361182679\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "63d6ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  count = 0\n",
    "  for sentence in nested_list:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "97c18691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 30 이하인 샘플의 비율: 94.31944999380003\n"
     ]
    }
   ],
   "source": [
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3c1e088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6d406461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145162, 30)\n",
      "(48745, 30)\n",
      "(145162,)\n",
      "(48745,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fa723c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = total_cnt  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 32  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feddaead",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "de291688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "284/284 [==============================] - 7s 8ms/step - loss: 0.4772 - accuracy: 0.7886 - val_loss: 0.3695 - val_accuracy: 0.8398\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 0.3338 - accuracy: 0.8596 - val_loss: 0.3681 - val_accuracy: 0.8408\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 2s 5ms/step - loss: 0.3069 - accuracy: 0.8730 - val_loss: 0.3643 - val_accuracy: 0.8419\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 0.2900 - accuracy: 0.8803 - val_loss: 0.3676 - val_accuracy: 0.8420\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 0.2767 - accuracy: 0.8868 - val_loss: 0.3773 - val_accuracy: 0.8405\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 0.2634 - accuracy: 0.8930 - val_loss: 0.3848 - val_accuracy: 0.8400\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 0.2494 - accuracy: 0.8993 - val_loss: 0.3911 - val_accuracy: 0.8408\n",
      "Epoch 8/10\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 0.2353 - accuracy: 0.9050 - val_loss: 0.4125 - val_accuracy: 0.8393\n",
      "Epoch 9/10\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 0.2215 - accuracy: 0.9107 - val_loss: 0.4379 - val_accuracy: 0.8384\n",
      "Epoch 10/10\n",
      "284/284 [==============================] - 2s 5ms/step - loss: 0.2085 - accuracy: 0.9168 - val_loss: 0.4611 - val_accuracy: 0.8373\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model1.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model1.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "hist = model1.fit(X_train,y_train,epochs=epochs,batch_size=512,\n",
    "                    validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])\n",
    "\n",
    "results1 = model1.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8c5ad",
   "metadata": {},
   "source": [
    "### 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e47abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model2.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model2.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model2.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model2.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "hist = model2.fit(X_train,y_train,epochs=epochs,batch_size=512,\n",
    "                    validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c88fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])\n",
    "\n",
    "results1 = model1.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d36efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model1.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model1.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "hist = model1.fit(X_train,y_train,epochs=epochs,batch_size=512,\n",
    "                    validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bf0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])\n",
    "\n",
    "results1 = model1.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53c9d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e31193",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model1.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model1.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "hist = model1.fit(X_train,y_train,epochs=epochs,batch_size=512,\n",
    "                    validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbeb608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model2.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65eb7da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()\n",
    "vocab_size = total_cnt  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 32  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model2.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model2.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model2.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model2.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206874df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = total_cnt  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 32  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model3.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model3.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model3.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6be95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model1.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c53a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77421017",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "hist = model2.fit(X_train,y_train,epochs=epochs,batch_size=512,\n",
    "                    validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a648f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model1.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93338a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6d0d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "hist = model3.fit(X_train,y_train,epochs=epochs,batch_size=512,\n",
    "                    validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model1.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4ae8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,4)\n",
    "plt.subplot(121)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss','val_loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb47abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
