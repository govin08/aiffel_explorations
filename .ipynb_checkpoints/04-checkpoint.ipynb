{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e98188",
   "metadata": {},
   "source": [
    "루브릭\n",
    "\n",
    "1. 시계열의 안정성이 충분히 확인되었는가?  \n",
    "플로팅과 adfuller 메소드 사용으로 시계열의 안정성이 (충분히) 확인되는 것을 시각화하였으며, 이에 대한 충분한 설명(해당 메소드에 대한 이해)이 서술되어있음\n",
    "- <font color = 'blue'>STEP 3</font>\n",
    "- <font color = 'blue'>**정성적 stationarity 분석**</font>\n",
    "    - 데이터의 stationarity가 보장되려면, 특정한 window size가 주어졌을 때, window의 평균(이동평균)과 표준편차 등이 시간에 의존하지 않고 일정해야 합니다.\n",
    "     - 이를 위해, `plot_rolling_statistics` 함수를 사용해 이동평균과 표준편차의 그래프를 플로팅하고, 정성적으로 stationarity를 분석했습니다.\n",
    "     - 이때 window size는 우리가 정할 수 있는 숫자이므로, 이 숫자를 바꿔가면서 실험해볼 수 있습니다.\n",
    "- <font color = 'blue'>**정량적 stationarity 분석**</font>\n",
    "     - stationarity를 판단하는 대표적인 지표는 p-value입니다.\n",
    "     - 더 정확하게 말하면, \"이 time series가 stationary하다\"라는 귀무가설에 대한 p-value를 의미합니다.\n",
    "     - p-value가 0.05 이하가 되면, 95% 이상의 confidence로 이 time series가 stationary하다고 말할 수 있습니다.\n",
    "     - p-value를 계산할 때에는, adfuller 메소드를 사용하여 정의한 `augmented_dickey_fuller_test`함수를 사용했습니다.\n",
    "     - 원래의 데이터를 (1) 로그변환, (2) 시계열 분해(추세 상쇄하기, 계절성 상쇄하기) 등을 통해 가공하면서, p-value를 측정해 stationarity가 보장되는지를 확인합니다.\n",
    "     \n",
    " -  조사한 세 개의 주식 종목에 대해서는 (1), (2)의 가공을 거치고 나면 p-value가 0.05보다 훨씬 적은 값을 가진다는 것을 확인할 수 있었습니다.\n",
    "\n",
    "2. ARIMA 모델 모수선택 근거를 체계적으로 제시하였는가?  \n",
    "p,q를 위한 ACF, PACF 사용과 d를 위한 차분 과정이 명확히 제시됨\n",
    "  - <font color = 'red'>STEP 5</font>\n",
    "    - 라이브러리`statsmodels.graphics.tsaplots`에서 제공하는 `plot_acf`, `plot_pacf` 함수를 사용하여, 로그변환까지 진행한 time series에 대하여 ACF(autocorrelation function)의 그래프와 PACF(partial autocorrelation function)의 그래프를 그렸습니다.\n",
    "  - <font color = 'red'> ACF, PACF를 이용해 p, q 결정하기</font>\n",
    "    - ACF와 PACF는 아래의 두가지 양상 중 하나의 양상으로 나타나는 경향이 있다고 합니다.\n",
    "    - (a) ACF가 exponentially decrease하거나 sinusoidally decrease하고, PACF가 처음의 n개에서 significant spike를 가지고 이후에는 가지지 않는 경우\n",
    "    - (b) PACF가 exponentially decrease하거나 sinusoidally decrease하고, ACF가 처음의 n개에서 significant spike를 가지고 이후에는 가지지 않는 경우\n",
    "    - (a)의 경우, `p=n`, `q=0`으로 정합니다. (b)의 경우, `p=0`, `q=n`으로 정합니다.\n",
    "  - <font color = 'red'> 차분안정성을 이용해 d 결정하기</font>\n",
    "    - 시계열은 하나의 수열로 생각할 수 있다. 이 수열의 계차수열을 생각하면, 그것은 원래 시계열의 일차 차분(`diff_1`)이다.\n",
    "    - 마찬가지로 이차차분(`diff_2`)도 생각할 수 있다.\n",
    "    - 원래의 시계열이 stationary하지 않을 경우에 일차차분과 이차차분을 구하여, stationary한 데이터를 이끌어내도록 한다.\n",
    "    - 만약 일차차분이 제일 stationary하면 d=1로, 이차차분이 제일 stationary하면 d=2로 놓는다.\n",
    "    - 하지만, 이것들은 다분히 이론적인 것이고, 실제로는 여러 번 시험해보고 가장 stationary한 값을 d의 값으로 정하면 된다.\n",
    "  - <font color = 'red'>p, d, q를 정하는 것에 관해서는 여러 자료들을 찾아보았다.</font>\n",
    "    - [8.17: How to pick the value of p in ARIMA models using ACF & PACF?](https://www.youtube.com/watch?v=_nSvoCkodS8)\n",
    "    - [8.18: How to pick the value of q in ARIMA models using ACF & PACF?](https://youtu.be/a0BVTH86JrI)\n",
    "    - [Quick way to find p, d and q values for ARIMA](https://analyticsindiamag.com/quick-way-to-find-p-d-and-q-values-for-arima/)\n",
    "    - [statsmodels.tsa.arima.model.ARIMA](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMA.html)\n",
    "    - [시계열 분석- ARIMA 모델](https://velog.io/@sjina0722/%EC%8B%9C%EA%B3%84%EC%97%B4%EB%B6%84%EC%84%9D-ARIMA-%EB%AA%A8%EB%8D%B8)\n",
    "\n",
    "3. 예측 모델의 오차율이 기준 이하로 정확하게 나왔는가?  \n",
    "  3개 이상 종목이 MAPE 15% 미만의 정확도로 예측됨\n",
    "  - <font color = 'green'>세 개의 종목에 대하여 모두 15% 이하의 MAPE 값이 나타납니다.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de793a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-datareader\n",
      "  Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "     -------------------------------------- 109.5/109.5 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from pandas-datareader) (1.5.0)\n",
      "Collecting requests>=2.19.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: lxml in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from pandas-datareader) (4.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from pandas>=0.23->pandas-datareader) (1.23.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from pandas>=0.23->pandas-datareader) (2022.2.1)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.5/61.5 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.4/140.4 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from requests>=2.19.0->pandas-datareader) (2022.9.14)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas-datareader) (1.16.0)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, requests, pandas-datareader\n",
      "Successfully installed charset-normalizer-2.1.1 idna-3.4 pandas-datareader-0.10.0 requests-2.28.1 urllib3-1.26.12\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42796edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.1.74-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from yfinance) (2.28.1)\n",
      "Requirement already satisfied: lxml>=4.5.1 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from yfinance) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from yfinance) (1.23.3)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from pandas>=0.24.0->yfinance) (2022.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from requests>=2.26->yfinance) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from requests>=2.26->yfinance) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from requests>=2.26->yfinance) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\govin\\anaconda3\\envs\\ex\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.24.0->yfinance) (1.16.0)\n",
      "Installing collected packages: multitasking, yfinance\n",
      "Successfully installed multitasking-0.0.11 yfinance-0.1.74\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5748e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pandas_datareader.data as pdr\n",
    "import yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10a189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_statistics(timeseries, window=12):\n",
    "    \n",
    "    rolmean = timeseries.rolling(window=window).mean()  # 이동평균 시계열\n",
    "    rolstd = timeseries.rolling(window=window).std()    # 이동표준편차 시계열\n",
    "\n",
    "     # 원본시계열, 이동평균, 이동표준편차를 plot으로 시각화해 본다.\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')    \n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5b31e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstattools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adfuller\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugmented_dickey_fuller_test\u001b[39m(timeseries):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# statsmodels 패키지에서 제공하는 adfuller 메서드를 호출합니다.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     dftest \u001b[38;5;241m=\u001b[39m adfuller(timeseries, autolag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIC\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def augmented_dickey_fuller_test(timeseries):\n",
    "    # statsmodels 패키지에서 제공하는 adfuller 메서드를 호출합니다.\n",
    "    dftest = adfuller(timeseries, autolag='AIC')  \n",
    "    \n",
    "    # adfuller 메서드가 리턴한 결과를 정리하여 출력합니다.\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cd66b",
   "metadata": {},
   "source": [
    "## 1. Apple 주가 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d858dfd",
   "metadata": {},
   "source": [
    "- `ts_log`\n",
    "  - 정성적 stationarity 분석\n",
    "    - window의 표준편차는 대체로 일정한 값을 유지하고 있지만, window의 평균(이동평균)은 조금씩 증가하는 경향을 보인다.\n",
    "    - 따라서, `ts_log`는 stationary하지 않다고 생각할 수 있다.\n",
    "  - 정량적 stationarity 분석\n",
    "    - p-value는 0.80 정도로 나타난다.\n",
    "    - 우리가 설정한 threshold인 0.05보다 훨씬 큰 값을 보이므로, `ts_log`는 전혀 stationary하지 않다고 말할 수 있다.\n",
    "- `residual`\n",
    "  - 정량적 stationarity 분석\n",
    "    - p-value는 2.49e-16 정도로 나타난다.\n",
    "    - 0.05보다 훨씬 작은 값을 보이므로, `residual`은 stationary하다고 말할 수 있다.\n",
    "- `p`, `q` 선택\n",
    "  - ACF의 그래프가 exponentially decrease한다고 정확하게 말하기는 어렵지만, 그렇게 가정하면, PACF 그래프가 두 개의 significant spike를 가지므로 `p=2`, `q=0`으로 두는 것이 맞을 것이다.\n",
    "  그런데 `p=3`, `q=0`의 경우가 실험적으로 더 좋은 성능을 내므로 `p=3`, `q=0`로 둔다.\n",
    "- `d` 선택\n",
    "  - d=1과 d=2를 실험해봤을 때, d=1의 경우가 더 성능이 좋으므로 d=1을 선택한다.\n",
    "- MAPE는 13.94%로 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a36c8",
   "metadata": {},
   "source": [
    "### STEP 1 : 시계열 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdr.get_data_yahoo('AAPL')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ea8f2",
   "metadata": {},
   "source": [
    "### STEP 2 : 각종 전처리 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9110e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리\n",
    "ts = ts.interpolate(method='time')\n",
    "ts[ts.isna()]  # Time Series에서 결측치가 있는 부분만 Series로 출력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfd02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 변환 시도 \n",
    "ts_log = np.log(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b84397",
   "metadata": {},
   "source": [
    "### STEP 3 : 시계열 안정성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbfeab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "# 정성적 그래프 분석\n",
    "plot_rolling_statistics(ts_log, window=12)\n",
    "\n",
    "#정량적 Augmented Dicky-Fuller Test\n",
    "augmented_dickey_fuller_test(ts_log)\n",
    "\n",
    "#시계열 분해 (Time Series Decomposition)\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(ts_log, model='multiplicative', period = 30) \n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,12)\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_log, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf8548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residual.dropna(inplace=True)\n",
    "augmented_dickey_fuller_test(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523d248",
   "metadata": {},
   "source": [
    "### STEP 4 : 학습, 테스트 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = ts_log[:int(len(ts_log)*0.9)], ts_log[int(len(ts_log)*0.9):]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.grid(True)\n",
    "plt.plot(ts_log, c='r', label='training dataset')  # train_data를 적용하면 그래프가 끊어져 보이므로 자연스러운 연출을 위해 ts_log를 선택\n",
    "plt.plot(test_data, c='b', label='test dataset')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c24d3",
   "metadata": {},
   "source": [
    "### STEP 5 : 적정 ARIMA 모수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e21b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(ts_log)   # ACF : Autocorrelation 그래프 그리기\n",
    "plot_pacf(ts_log)  # PACF : Partial Autocorrelation 그래프 그리기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb6564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 차분 안정성 확인 -> d 구하기\n",
    "\n",
    "# 1차 차분 구하기\n",
    "diff_1 = ts_log.diff(periods=1).iloc[1:]\n",
    "diff_1.plot(title='Difference 1st')\n",
    "\n",
    "augmented_dickey_fuller_test(diff_1)\n",
    "\n",
    "# 혹시 필요한 경우 2차 차분 구하기\n",
    "diff_2 = diff_1.diff(periods=1).iloc[1:]\n",
    "diff_2.plot(title='Difference 2nd')\n",
    "\n",
    "augmented_dickey_fuller_test(diff_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae1b0f",
   "metadata": {},
   "source": [
    "### STEP 6 : ARIMA 모델 훈련과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "# Build Model\n",
    "model = ARIMA(train_data, order=(3, 1, 0))  \n",
    "fitted_m = model.fit() \n",
    "\n",
    "print(fitted_m.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b27c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forecast : 결과가 fc에 담깁니다. \n",
    "fc = fitted_m.forecast(len(test_data), alpha=0.05)  # 95% conf\n",
    "fc = np.array(fc)\n",
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index=test_data.index)   # 예측결과\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5), dpi=100)\n",
    "plt.plot(train_data, label='training')\n",
    "plt.plot(test_data, c='b', label='actual price')\n",
    "plt.plot(fc_series, c='r',label='predicted price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194a7d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "mse = mean_squared_error(np.exp(test_data), np.exp(fc))\n",
    "print('MSE: ', mse)\n",
    "\n",
    "mae = mean_absolute_error(np.exp(test_data), np.exp(fc))\n",
    "print('MAE: ', mae)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(np.exp(test_data), np.exp(fc)))\n",
    "print('RMSE: ', rmse)\n",
    "\n",
    "mape = np.mean(np.abs(np.exp(fc) - np.exp(test_data))/np.abs(np.exp(test_data)))\n",
    "print('MAPE: {:.2f}%'.format(mape*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2b4ac",
   "metadata": {},
   "source": [
    "## 2.S&P 500 주가 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5ba42",
   "metadata": {},
   "source": [
    "- `ts_log`\n",
    "  - 정성적 stationarity 분석\n",
    "    - window의 표준편차는 대체로 일정한 값을 유지하고 있고, window의 평균(이동평균)은 일정한 구간 안에서 횡보하는 경향을 보인다.\n",
    "    - 따라서, `ts_log`는 stationary하지 않다고 생각할 수 있다.\n",
    "  - 정량적 stationarity 분석\n",
    "    - p-value는 0.52 정도로 나타난다.\n",
    "    - 이 값은 `AAPL`의 경우보다는 적은 값이기는 하지만, 0.05보다 훨씬 큰 값을 보이므로, `ts_log`는 stationary하지 않다고 말할 수 있다.\n",
    "- `residual`\n",
    "  - 정량적 stationarity 분석\n",
    "    - p-value는 6.42e-19 정도로 나타난다.\n",
    "    - 0.05보다 훨씬 작은 값을 보이므로, `residual`은 stationary하다고 말할 수 있다.\n",
    "- `p`, `q` 선택\n",
    "  - ACF의 그래프가 exponentially decrease한다고 정확하게 말하기는 어렵지만, 그렇게 가정하면, PACF 그래프가 2-4개의 significant spike를 가지므로 `p=2, 3, 4`, `q=0`으로 두는 것이 맞을 것이다.\n",
    "  그런데 `p=4`인 경우가 실험적으로 가 좋은 성능을 내므로 `p=4`, `q=0`로 둔다.\n",
    "- `d` 선택\n",
    "  - d=1과 d=2를 실험해봤을 때, d=1의 경우가 더 성능이 좋으므로 d=1을 선택한다.\n",
    "- MAPE는 12.94%로 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a261001",
   "metadata": {},
   "source": [
    "### STEP 1 : 시계열 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f94aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdr.get_data_yahoo('^GSPC')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892dcea",
   "metadata": {},
   "source": [
    "### STEP 2 : 각종 전처리 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc921b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리\n",
    "ts = ts.interpolate(method='time')\n",
    "ts[ts.isna()]  # Time Series에서 결측치가 있는 부분만 Series로 출력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02919c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 변환 시도 \n",
    "ts_log = np.log(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ecaed3",
   "metadata": {},
   "source": [
    "### STEP 3 : 시계열 안정성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b595c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "# 정성적 그래프 분석\n",
    "plot_rolling_statistics(ts_log, window=12)\n",
    "\n",
    "#정량적 Augmented Dicky-Fuller Test\n",
    "augmented_dickey_fuller_test(ts_log)\n",
    "\n",
    "#시계열 분해 (Time Series Decomposition)\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(ts_log, model='multiplicative', period = 30) \n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,12)\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_log, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24296046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residual.dropna(inplace=True)\n",
    "augmented_dickey_fuller_test(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317ace2",
   "metadata": {},
   "source": [
    "### STEP 4 : 학습, 테스트 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ecd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = ts_log[:int(len(ts_log)*0.9)], ts_log[int(len(ts_log)*0.9):]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.grid(True)\n",
    "plt.plot(ts_log, c='r', label='training dataset')  # train_data를 적용하면 그래프가 끊어져 보이므로 자연스러운 연출을 위해 ts_log를 선택\n",
    "plt.plot(test_data, c='b', label='test dataset')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19226688",
   "metadata": {},
   "source": [
    "### STEP 5 : 적정 ARIMA 모수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(ts_log)   # ACF : Autocorrelation 그래프 그리기\n",
    "plot_pacf(ts_log)  # PACF : Partial Autocorrelation 그래프 그리기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b3492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 차분 안정성 확인 -> d 구하기\n",
    "\n",
    "# 1차 차분 구하기\n",
    "diff_1 = ts_log.diff(periods=1).iloc[1:]\n",
    "diff_1.plot(title='Difference 1st')\n",
    "\n",
    "augmented_dickey_fuller_test(diff_1)\n",
    "\n",
    "# 혹시 필요한 경우 2차 차분 구하기\n",
    "diff_2 = diff_1.diff(periods=1).iloc[1:]\n",
    "diff_2.plot(title='Difference 2nd')\n",
    "\n",
    "augmented_dickey_fuller_test(diff_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4b169",
   "metadata": {},
   "source": [
    "### STEP 6 : ARIMA 모델 훈련과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "# Build Model\n",
    "model = ARIMA(train_data, order=(3, 1, 0))  \n",
    "fitted_m = model.fit() \n",
    "\n",
    "print(fitted_m.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c441126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forecast : 결과가 fc에 담깁니다. \n",
    "fc = fitted_m.forecast(len(test_data), alpha=0.05)  # 95% conf\n",
    "fc = np.array(fc)\n",
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index=test_data.index)   # 예측결과\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5), dpi=100)\n",
    "plt.plot(train_data, label='training')\n",
    "plt.plot(test_data, c='b', label='actual price')\n",
    "plt.plot(fc_series, c='r',label='predicted price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df87c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "mse = mean_squared_error(np.exp(test_data), np.exp(fc))\n",
    "print('MSE: ', mse)\n",
    "\n",
    "mae = mean_absolute_error(np.exp(test_data), np.exp(fc))\n",
    "print('MAE: ', mae)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(np.exp(test_data), np.exp(fc)))\n",
    "print('RMSE: ', rmse)\n",
    "\n",
    "mape = np.mean(np.abs(np.exp(fc) - np.exp(test_data))/np.abs(np.exp(test_data)))\n",
    "print('MAPE: {:.2f}%'.format(mape*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d870ce0",
   "metadata": {},
   "source": [
    "## 3. Dow Jones 주가 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07c080",
   "metadata": {},
   "source": [
    "- `ts_log`\n",
    "  - 정성적 stationarity 분석\n",
    "    - window의 표준편차와 평균(이동평균) 모두 대체로 일정한 값을 유지하고 있다.\n",
    "    - 따라서, `ts_log`는 어느 정도는 stationary하다고 추측할 수 있다.\n",
    "  - 정량적 stationarity 분석\n",
    "    - p-value는 0.30 정도로 나타난다.\n",
    "    - 우리가 설정한 threshold인 0.05보다 큰 값을 보이므로, `ts_log`는 stationary하지 않다고 말할 수 있다.\n",
    "- `residual`\n",
    "  - 정량적 stationarity 분석\n",
    "    - p-value는 1.00e-19 정도로 나타난다.\n",
    "    - 0.05보다 훨씬 작은 값을 보이므로, `residual`은 stationary하다고 말할 수 있다.\n",
    "- `p`, `q` 선택\n",
    "  - ACF의 그래프가 exponentially decrease한다고 정확하게 말하기는 어렵지만, 그렇게 가정하면, PACF 그래프가 2-4 개의 significant spike를 가지므로 `p=2,3,4`, `q=0`으로 두는 것이 맞을 것이다.\n",
    "  그런데 `p=2`, `q=0`의 경우가 실험적으로 더 좋은 성능을 내므로 `p=2`, `q=0`로 둔다.\n",
    "- `d` 선택\n",
    "  - d=1과 d=2를 실험해봤을 때, d=1의 경우가 더 성능이 좋으므로 d=1을 선택한다.\n",
    "- MAPE는 8.30%로 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e8e21",
   "metadata": {},
   "source": [
    "### STEP 1 : 시계열 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ac1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdr.get_data_yahoo('^DJI')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68edac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991b831",
   "metadata": {},
   "source": [
    "### STEP 2 : 각종 전처리 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리\n",
    "ts = ts.interpolate(method='time')\n",
    "ts[ts.isna()]  # Time Series에서 결측치가 있는 부분만 Series로 출력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59545ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 변환 시도 \n",
    "ts_log = np.log(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e546be2",
   "metadata": {},
   "source": [
    "### STEP 3 : 시계열 안정성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5da86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "# 정성적 그래프 분석\n",
    "plot_rolling_statistics(ts_log, window=12)\n",
    "\n",
    "#정량적 Augmented Dicky-Fuller Test\n",
    "augmented_dickey_fuller_test(ts_log)\n",
    "\n",
    "#시계열 분해 (Time Series Decomposition)\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(ts_log, model='multiplicative', period = 30) \n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,12)\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_log, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec31f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residual.dropna(inplace=True)\n",
    "augmented_dickey_fuller_test(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20aeab5",
   "metadata": {},
   "source": [
    "### STEP 4 : 학습, 테스트 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = ts_log[:int(len(ts_log)*0.9)], ts_log[int(len(ts_log)*0.9):]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.grid(True)\n",
    "plt.plot(ts_log, c='r', label='training dataset')  # train_data를 적용하면 그래프가 끊어져 보이므로 자연스러운 연출을 위해 ts_log를 선택\n",
    "plt.plot(test_data, c='b', label='test dataset')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444f962",
   "metadata": {},
   "source": [
    "### STEP 5 : 적정 ARIMA 모수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3926cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(ts_log)   # ACF : Autocorrelation 그래프 그리기\n",
    "plot_pacf(ts_log)  # PACF : Partial Autocorrelation 그래프 그리기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf6551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 차분 안정성 확인 -> d 구하기\n",
    "\n",
    "# 1차 차분 구하기\n",
    "diff_1 = ts_log.diff(periods=1).iloc[1:]\n",
    "diff_1.plot(title='Difference 1st')\n",
    "\n",
    "augmented_dickey_fuller_test(diff_1)\n",
    "\n",
    "# 혹시 필요한 경우 2차 차분 구하기\n",
    "diff_2 = diff_1.diff(periods=1).iloc[1:]\n",
    "diff_2.plot(title='Difference 2nd')\n",
    "\n",
    "augmented_dickey_fuller_test(diff_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc4cf1",
   "metadata": {},
   "source": [
    "### STEP 6 : ARIMA 모델 훈련과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "# Build Model\n",
    "model = ARIMA(train_data, order=(2, 1, 0))  \n",
    "fitted_m = model.fit() \n",
    "\n",
    "print(fitted_m.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd79b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forecast : 결과가 fc에 담깁니다. \n",
    "fc = fitted_m.forecast(len(test_data), alpha=0.05)  # 95% conf\n",
    "fc = np.array(fc)\n",
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index=test_data.index)   # 예측결과\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5), dpi=100)\n",
    "plt.plot(train_data, label='training')\n",
    "plt.plot(test_data, c='b', label='actual price')\n",
    "plt.plot(fc_series, c='r',label='predicted price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2d1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "mse = mean_squared_error(np.exp(test_data), np.exp(fc))\n",
    "print('MSE: ', mse)\n",
    "\n",
    "mae = mean_absolute_error(np.exp(test_data), np.exp(fc))\n",
    "print('MAE: ', mae)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(np.exp(test_data), np.exp(fc)))\n",
    "print('RMSE: ', rmse)\n",
    "\n",
    "mape = np.mean(np.abs(np.exp(fc) - np.exp(test_data))/np.abs(np.exp(test_data)))\n",
    "print('MAPE: {:.2f}%'.format(mape*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc02ebe",
   "metadata": {},
   "source": [
    "**회고**\n",
    "\n",
    "1. trivia\n",
    "\n",
    "처음으로 회고를 쓴다.\n",
    "회고를 쓰는 것은, 공자시항에 '회고를 꼭 적어달라'는 요청이 있었기도 하고, 며칠 전 두 명의 선배(2022년 2기)분들이 추천하셨기 때문이기도 하다.\n",
    "\n",
    "지금까지 두 개의 exploration을 제출했다.\n",
    "하나 아쉬운 것은 exploration 3를 제출하지 못한 것 같다는 것이다.\n",
    "나는 제출일이 지난 주 토요일(9/24)까지인 줄 알고 있었는데, 이상하게 그날 LMS에 접속해보니, 제출일이 지났다고 떴다.\n",
    "전날은 시간도 없었거니와, 자주 LMS 서버가 다운되어서 제출하기 힘들었던 점도 있었는데, 이렇게 제출하지 못하게 되어 정말 아쉽다.\n",
    "\n",
    "지금까지 첫번째의 exploration에 대해서만 채점이 완료되었다.\n",
    "나는 별 두 개를 받았는데, 납득할 수 없는 점수였다.\n",
    "별을 못받은 항목에 대하여 내가 못한 것이 없다고 생각했기 때문이다.\n",
    "이의를 제기했고, 캠퍼스매니저님이 말씀하시길, 채점할 때에 404 error가 떴고, 대신 해당 깃허브 주소의 다른 파일들로 추적해 채점을 하였으며, 그 과정에서 최종본이 아닌 중간본을 채점하셨다는 것이다.\n",
    "나 말고도 몇몇 그루분들이 나와 같은 경우가 있었다.\n",
    "결론은, \"단 하나의 파일이 포함된, 안정적인 링크를 올려라\"라는 것이다.\n",
    "그리하여, exploration 4부터는 아예 새로운 곳에 올리기로 했다.\n",
    "정확하게는, repository를 새로 하나 파고 (`aiffel_explorations`), 거기에는 한 exploration당 하나의 파일만을, 숫자의 형태로 (e.g. `04.ipynb`) 올릴 것이다.\n",
    "그리고 이 파일이 `aiffel_explorations` repository의 첫번째 파일이다.\n",
    "\n",
    "모든 걸 작업해놓고 다 날렸다.\n",
    "수정사항이 github에 잘 안올라가기에, 깃헙의 repository를 삭제하고 다시 만들었는데, 그 과정에서 작업한 것이 다 날아갔다.\n",
    "LMS 서버에는 남아있을 거라고 생각했는데, 그렇지 않았다.\n",
    "이걸 하느라 밤을 샜는데 (밤을 새리라고는 생각하지 못했다, 거의 완성했다고 생각했기 때문이다.) 기억을 더듬어서 재빨리 복구했다. 그래도 머릿속에 과정들이 남아 있어서 다행이다.\n",
    "오늘은, 아니 어제는 데이터톤이 진행되었었다.\n",
    "\n",
    "2. ARIMA\n",
    "\n",
    "이 exploration 노드는, 지난 주 화요일엔 진행되었던 노드인데, 당일에 바로 프로젝트를 완성하지 못했다.\n",
    "완성하기는 커녕, 관련된 개념을 완전히 익히지도 못했다.\n",
    "이 노드의 핵심은 ARIMA 모델이다.\n",
    "\n",
    "나는 대학원에 있을 때, LSTM timeseries prediction을 해본 적이 있다.\n",
    "하지만, 나를 지도해주셨던 몇몇 교수님들은, 사실 해당 부분들에 대해서 아주 자세히 알고 계시지 못하였다.\n",
    "그러니까, 내가 혼자 헤쳐나가야 했었던 셈이다.\n",
    "그때 처음으로 ARIMA 모델을 본 적이 있다.\n",
    "정확하게는, GRU인가 LSTM모델이 특정 머신러닝 문제에 대하여, baseline인 ARIMA에 비해 괜찮은 성능을 보였다는 논문을 읽은 적이 있다.\n",
    "그래서, ARIMA 모델이라는 게 도대체 뭔가, 하고 의문을 가졌었다.\n",
    "학부 과정의 '시계열 분석'이라는 수업을 청강하기도 했었다. (하지만, 이내 두번인가 세 번만 듣고, 바빠서 더 듣지 못했고, 도움이 되지 못했다.)\n",
    "\n",
    "그리고 시간이 지나, 이번 기회에 aiffel 노드에서 ARIMA 모델에 대해 공부하게 되었다.\n",
    "솔직히 말하면, 아직도 잘 모르겠다.\n",
    "이것을 정확하게 알려면, 내가 청강하려고 했었던 그 수업을 열심히 들을 정도의 노력을 기울여야 정확히 이해할 것이라고 생각된다.\n",
    "하지만 나는 단지, 단 사흘 정도를 고민했으니(지난 주 화요일과 토요일 그리고 오늘) 완전히 이해가 안 가는 것도 당연할지도 모르겟다.\n",
    "\n",
    "그래도, 조금은 알게 되었다.\n",
    "ARIMA 모델은 세 개의 submodel AR, I, MA을 결합한 모델이라는 것(정확한 표현이 아닐 수 있다.), 그리고 각각의 submodel들에는 특정한 인자들 p, d, q가 지정된다는 것 등이다.\n",
    "\n",
    "3. ACF, PACF, p, q\n",
    "\n",
    "ACF와 PACF가 뭔지는 아직도 전혀 모르겠지만, 그래도 p, q를 결정하는 방법은 다음과 같다.\n",
    "ACF가 exponentially decrease하거나 sinusoidally decrease하면 q=0으로 둔 채로 p값을 정한다.\n",
    "p값을 정할 때에는 PACF에서 significant spike들의 개수를 세어 그것을 p값으로 정한다.\n",
    "반대로, PACF가 exponentially decrease하거나 sinusoidally decrease하면 p=0으로 둔 채로 q값을 정한다.\n",
    "q값을 정할 때에는, 마찬가지로 ACF에서 significant spike들의 개수를 세어 그것을 q값으로 정한다.\n",
    "\n",
    "이상한 것은, 내가 해본 모든 주식자료들은 모두 ACF가 exponentially decrease한다는 것이다.\n",
    "정확히 말하면, ACF를 exponentially decrease한다고 말하는 것 외에 다른 표현방법을 찾지 못했다.\n",
    "그러니까 exponentailly decrease한다고 말하기에는 decay가 너무 적어보이는데, 이 두 그래프를 달리 해석할 방법이 없어서, 나는 그냥 ACF를 exponentially decrease한다고 판단한 것이다.\n",
    "어쨌든 적어도 ACF가 sinusoidally decrease하지는 않고, 맨 위에 적은 두 개의 케이스 (a) q=0, (b) p=0 중에서 (b)의 케이스는 아닌 것으로 판단하고 있다.\n",
    "\n",
    "하지만, 임의로 (b) p=0으로 설정하여 ARIMA 모델을 돌리더라도, 크게 MAPE 값이 변하지는 않는다. 심지어는 (a)의 경우로 판단했는데도 불구하고 실험적으로는 p=0으로 두는 것이 MAPE 값이 더 잘나오는 경우도 있었다.\n",
    "\n",
    "그래도 대체로는, p, q를 구하는 criteria를 따르는 것이 괜찮은 MAPE 값을 얻는 방법이라고 여겨진다.\n",
    "\n",
    "4. differencing, d\n",
    "\n",
    "또하나의 인자인 d는 차분(differencing)과 관련되어 있다.\n",
    "이것은 미분의 개념과는 조금 다르지만, 비슷하게 생각할 수 있겠다.\n",
    "조금 더 정확하게는, 함수의 미분이라기보다는, 수열에 대한 계차수열 개념이라고 보면 더 정확하겠다.\n",
    "응용수학 분야에서는 미분의 개념을 뺄셈의 개념으로 치환하는 경우가 종종 있는 것 같은데, 그 한 예가 이것인 것 같다.\n",
    "\n",
    "d=1은 주어진 time series를 한번 차분한 것이다.\n",
    "그러니 d=2는 차분을 한번 더 한 것이다.\n",
    "사실, d=0으로 설정하고 ARIMA 모델을 돌리는 것도 가능했었는데, d=0이 성능이 더 잘 나오는 경우도 있었다.\n",
    "하지만 d=0은 루브릭의 instruction에 없었으므로 d=1과 d=2만 시행했다.\n",
    "그리고, d=1인 경우가 항상 d=2인 경우보다 잘 나오는 모습을 보였다.\n",
    "\n",
    "5. 기타\n",
    "\n",
    "데이터를 불러오는 데에는 `yhfinance`와 `pandas_datareader`를 사용했다.\n",
    "이 두 모듈은 LMS 서버에 저장되어 있지 않았기 때문에 따로 다운받아야 했다.\n",
    "이것들을 사용하게 된 데에는, 몇 주 전 경원 퍼실님께서 주식 데이터 미션을 주셨던 것과 연관이 있다.\n",
    "이것들을 이전에 다루어본 적은 있지만, 퍼실님께서 미션을 주셨을 때에 한 번 다시 연습해봤기 때문에 망설임 없이 사용할 수 있었다.\n",
    "\n",
    "예측 결과들은, 사실 만족스러워보이지 않는다.\n",
    "예측된 값들은 이상하게 linear한 형태로만 예측된다.\n",
    "LMS에서는 조금 더 깔끔한 형태로 예측되기도 했었던 것 같은데, 왜 여기서는 그렇게 되지 않는지 아직 잘 모르겠다.\n",
    "\n",
    "밤새서 이 자료를 만들었는데, 한 번 다 날렸다. 다행히 거의 다 복구해가고 있는 듯하다.\n",
    "\n",
    "6. 결과요약.\n",
    "\n",
    "결과를 간략하게 표로 요약해보자.\n",
    "\n",
    "|section |종목    |(p,d,q)  |MAPE  |비고|\n",
    "|     :-:|     :-:|      :-:|   :-:| :-:|\n",
    "|1       |애플    |(3,1,0)  |13.94%|성공|\n",
    "|2       |S&P500  |(4,1,0)  |12.94%|성공|\n",
    "|3       |다우존스|(2,1,0)  | 8.30%|성공|\n",
    "\n",
    "7. p-value\n",
    "\n",
    "p-value의 진정한 의미에 대해서는, 처음에는 잘 이해하지 못하다가, 루브릭 1을 만족시키는 자료를 만들려다 보니 이해하게 되었다.\n",
    "\n",
    "사실 이전부터 확률과정(stochastic process)에서의 stationarity 같은 건 들어본 적이 있다.\n",
    "나는 확률과정개론같은 걸 열심히 공부해본 적이 없었으므로 깊게 파본 것은 전혀 아니고 정말 지나가다가 들어본 적이 있다.\n",
    "그러한 stationarity를 수학적으로 정의하는 것은 의미있는 일이지만, 그것은 차차 나중에 공부하기로 하고, stationarity를 정성적으로 정의하는 방식이 흥미로웠다.\n",
    "요는, stationarity가 통계적인 여러 모수(e.g. 평균, 분산, autocorrelation)들이, 시간에 depend하지 않는 것에 관련된 개념이라는 것이다.\n",
    "\n",
    "이러한 stationarity를 판단하기 위한 판정법이 소개되었다.\n",
    "그것은 귀무가설과 대립가설을 이용한 판정법이다.\n",
    "\"주어진 time series가 stationary하지 않다\"라는 귀무가설을 세운 후, 이 귀무가설이 신빙성이 있느냐 하는 정도를 p-value로 표현했다.\n",
    "다시 말하면, p-value의 값이 적은 것이 주어진 time series가 stationary하다는 사실에 대응된다.\n",
    "우리는 여러 전처리를 통해서 time series를 stationary하게 만들려고 노력하고, 그 과정에서 p-value를 계속 체크한다.\n",
    "마침내 p-value가 특정 값(threshold, 0.05)보다 작게 측정되었을 때, 비로소 귀무가설을 폐기하고 대립가설인 \"times series가 stationary하다\"을 차용할 수 있는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7684dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
